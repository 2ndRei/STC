{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.io import wavfile\n",
    "import scipy.fftpack as fftpack\n",
    "import scipy.signal.windows as windows\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = '.\\data_v_7_stc'\n",
    "FS = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR = os.path.join(DATASET_PATH, 'audio')\n",
    "train_file = [f for f in os.listdir(TRAIN_DIR) if os.path.isfile(os.path.join(TRAIN_DIR, f))]\n",
    "train_wave = [w for w in train_file if w[-4:] == '.wav']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATASET_PATH, 'meta/meta.txt'), sep='\\t', names=['File', '_', '__', 'Duration', 'Class'])\n",
    "df.drop(['_', '__'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEST_DIR = os.path.join(DATASET_PATH, 'test')\n",
    "test_file = [f for f in os.listdir(TEST_DIR) if os.path.isfile(os.path.join(TEST_DIR, f))]\n",
    "test_wave = [w for w in test_file if w[-4:] == '.wav']\n",
    "\n",
    "test_tag_split = [(f.split('_'))[:-1] for f in test_wave]\n",
    "test_tag_list = [ [tag for tag in tag_split if tag != 't'] for tag_split in test_tag_split]\n",
    "test_tag = ['_'.join(tag_list) for tag_list in test_tag_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(np.concatenate((test_wave, test_tag)).reshape(-1, 2, order='F'), columns=['File', 'Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags = np.unique(test_tag)\n",
    "d_class = dict(zip(tags, range(tags.shape[0])))\n",
    "d_class_inv = {val:key for key, val in d_class.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preproc_wavfile(full_file_name):\n",
    "    fs, data = wavfile.read(full_file_name)\n",
    "    data_scaled = signal_scaler(data)\n",
    "    signal_part = signal_detector(data_scaled, fs//10, fs//10, 10)\n",
    "    \n",
    "    return data_scaled, fs, signal_part\n",
    "\n",
    "def signal_scaler(data, maximum=2**15 - 1):\n",
    "    scale = maximum / max(abs(data))\n",
    "    \n",
    "    return np.array(data*scale, dtype=data.dtype)\n",
    "\n",
    "def std_average(data, window):\n",
    "    N = data.shape[0]\n",
    "    W = int(window)\n",
    "    HW = int(window // 2)\n",
    "    if (N <= W):\n",
    "        return np.array([data.mean()], dtype=data.dtype)\n",
    "    \n",
    "    data_std  = np.zeros(int((2*(N//W + 1)),), dtype=data.dtype)\n",
    "    \n",
    "    data_std[0] = data[:HW].std()\n",
    "    for i in range(data_std.shape[0] - 2):\n",
    "        data_std[i+1] = data[i*HW:(i+2)*HW].std()\n",
    "    data_std[-1] = data[-HW:].std()\n",
    "\n",
    "    return data_std\n",
    "\n",
    "def signal_detector(data, window, min_duration_frame, scale=10):\n",
    "    data_std = std_average(data, window)\n",
    "    \n",
    "    std_filter = np.zeros(data_std.shape, dtype='bool')\n",
    "    std_filter[data_std > max(data_std)/scale] = True\n",
    "       \n",
    "    while (all(std_filter)):\n",
    "        scale /= 2\n",
    "        std_filter = np.zeros(data_std.shape, dtype='bool')\n",
    "        std_filter[data_std > max(data_std)/scale] = True\n",
    "        \n",
    "    if (not any(std_filter)):\n",
    "        return []\n",
    "    \n",
    "    HW = window//2\n",
    "    signal_frame = []\n",
    "    front = np.argwhere(std_filter).reshape(-1,)[0]\n",
    "    back = 0\n",
    "    is_signal = std_filter[0]\n",
    "    signal_duration = 0\n",
    "    silence_duration = 0\n",
    "    for i, flag in enumerate(std_filter):\n",
    "        if (flag and not is_signal):\n",
    "            silence_duration = (i - back + 1)*HW\n",
    "            is_signal = True\n",
    "            if (silence_duration > min_duration_frame):\n",
    "                if (signal_duration >= min_duration_frame):\n",
    "                    signal_frame.append((max(0,             (front-1)*HW),\n",
    "                                         min(data.shape[0], (back   )*HW)))\n",
    "                front = i     \n",
    "        if (not flag and is_signal):\n",
    "            back = i\n",
    "            is_signal = False\n",
    "            signal_duration = (back - front + 1)*HW\n",
    "            \n",
    "    if (std_filter[-1]):\n",
    "        signal_frame.append((max(0,(front-1)*HW), data.shape[0]))\n",
    "    else:\n",
    "        signal_frame.append((max(0,             (front-1)*HW),\n",
    "                             min(data.shape[0], (back   )*HW)))     \n",
    "    \n",
    "    return signal_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_oscilogram_feature(DIR, file_name_list):\n",
    "    signal_parts = [None] * len(file_name_list)\n",
    "    feat_ndarray = np.zeros((len(file_name_list), 4))\n",
    "    for i, file_name in enumerate(file_name_list):\n",
    "        data_scaled, _, signal_part = preproc_wavfile(os.path.join(DIR, file_name))\n",
    "        feat_ndarray[i] = oscilogram_feature(data_scaled, signal_part)\n",
    "        signal_parts[i] = signal_part\n",
    "        \n",
    "    return feat_ndarray, ['std_median', 'power_median', 'ratio', 'part_count'], signal_parts\n",
    "\n",
    "def oscilogram_feature(data_scaled, signal_part):\n",
    "    if (len(signal_part) == 0):\n",
    "        return np.std(data_scaled), np.median(data_scaled**2), 1, 0\n",
    "    \n",
    "    N = data_scaled.shape[0]\n",
    "    signal_data = np.ndarray((0,))\n",
    "    signal_duration = 0\n",
    "    for front, back in signal_part:\n",
    "        signal_data = np.concatenate((signal_data, data_scaled[front:back]))\n",
    "        signal_duration += back - front\n",
    "    std_median = np.std(signal_data)\n",
    "    power_median = np.median(signal_data**2)\n",
    "    ratio = signal_duration / N\n",
    "    part_count = len(signal_part)\n",
    "    \n",
    "    return std_median, power_median, ratio, part_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_oscilo_feat, _, _ = process_oscilogram_feature(TRAIN_DIR, df['File'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_oscilo_feat, _, _ = process_oscilogram_feature(TEST_DIR, df_test['File'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def moving_average(data, window):\n",
    "    s = pd.Series(data).rolling(window=window).mean()\n",
    "    s = s.shift(-window//2).dropna().tolist()\n",
    "    \n",
    "    return s[:1]*(window//2) + s + s[-1:]*(window//2 - 1)\n",
    "\n",
    "def exponential_smoothing(data, alpha):\n",
    "    result = [data[0]] # first value is same as series\n",
    "    for n in range(1, len(data)):\n",
    "        result.append(alpha * data[n] + (1 - alpha) * result[n-1])\n",
    "        \n",
    "    return result\n",
    "\n",
    "def fft(data, fs, func_avr=None, *param):\n",
    "    data = np.array(data)\n",
    "    if (data.shape[0] % fs != 0):\n",
    "        data = np.concatenate((data, np.zeros((fs - data.shape[0] % fs, ) , dtype=data.dtype)), axis=0)\n",
    "    \n",
    "    N = data.shape[0]\n",
    "    fn = fs//2  \n",
    "    n = N//2\n",
    "    \n",
    "    X = fftpack.fft(data) / (N / 2.0)\n",
    "    mag = np.abs(X / abs(X).max())[:n]\n",
    "    X_db = 20*np.log10(mag)\n",
    "    freq = fftpack.fftfreq(N, 1/fs)[:n]\n",
    "\n",
    "    spec = X_db.reshape(fn, -1).mean(axis=1)\n",
    "    if (func_avr is not None):\n",
    "        spec = np.array(func_avr(spec, *param))\n",
    "    \n",
    "    return (spec, list(range(1, fn + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_spectral_feature(DIR, file_name_list, sample_rate=FS):\n",
    "    signal_parts = [None] * len(file_name_list)\n",
    "    feat_ndarray = np.zeros((len(file_name_list), sample_rate//2))\n",
    "    for i, file_name in enumerate(file_name_list):\n",
    "        data_scaled, fs, signal_part = preproc_wavfile(os.path.join(DIR, file_name))\n",
    "        feat, freq = spectrum_feature(data_scaled, fs, signal_part)\n",
    "        feat_ndarray[i], frequency_range = feat[:sample_rate//2], freq[:sample_rate//2]\n",
    "        signal_parts[i] = signal_part\n",
    "        \n",
    "    return feat_ndarray, frequency_range, signal_parts\n",
    "\n",
    "def spectrum_feature(data, fs, signal_part):\n",
    "    if (len(signal_part) == 0):\n",
    "        window = windows.hann(data.shape[0])\n",
    "        return fft(window*data, fs, moving_average, fs//100)\n",
    "\n",
    "    N = data.shape[0]\n",
    "    signal_data = np.ndarray((0,))\n",
    "    for front, back in signal_part:\n",
    "        Xn = data[front:back]\n",
    "        Xn_1 = np.roll(Xn, 1)\n",
    "        Xn_1[0] = Xn_1[1]\n",
    "        window = windows.hann(back - front)\n",
    "        signal_data = np.concatenate((signal_data, window*(Xn - 0.95*Xn_1)))\n",
    "        \n",
    "    return fft(signal_data, fs, moving_average, fs//100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_spectr_feat, _, _ = process_spectral_feature(TRAIN_DIR, df['File'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_spectr_feat, _, _ = process_spectral_feature(TEST_DIR, df_test['File'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(mode_list):\n",
    "    X_train = np.ndarray((df.shape[0], 0))\n",
    "    X_test = np.ndarray((df_test.shape[0], 0))\n",
    "        \n",
    "    for mode in mode_list:\n",
    "        if (mode == 'oscilo'):\n",
    "            X_train = np.concatenate((X_train, train_oscilo_feat), axis=1)\n",
    "            X_test = np.concatenate((X_test, test_oscilo_feat), axis=1)\n",
    "        elif (mode == 'spec'):\n",
    "            X_train = np.concatenate((X_train, train_spectr_feat), axis=1)\n",
    "            X_test = np.concatenate((X_test, test_spectr_feat), axis=1)\n",
    "        \n",
    "    y_train = (df['Class'].map(d_class)).values\n",
    "    y_test = (df_test['Class'].map(d_class)).values\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все исследования проводятся на пятикратной кросс-валидации тренировочных данных.\n",
    "\n",
    "Подбор гипперпараметров для классификаторов осуществлен в RnD.ipynb\n",
    "\n",
    "Далее идет подбор гипперпараметров (alpha, beta, gamma) для ансамбля классификаторов (занимает порядка 5 часов).\n",
    "\n",
    "В завершении подбирается порог отсечения класса \"unknown\" для открытой задачи классификации (open_threshold). Он рассчитывается как медианное значение вероятности распознавания неверного класса на всей кросс-валидации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как процесс подбора гипперпараметров для ансамбля классификаторов занимает много времени, нижеследующей ячейкой можно задать заранее мной полученные для них значения и порог отсечения класса \"unknown\" для открытой задачи классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mode_list = ['oscilo', 'spec']\n",
    "\n",
    "hyper_params = {}\n",
    "param_grid = np.linspace(0.0, 1.0, 11)\n",
    "for key, index in zip(('alpha', 'beta', 'gamma'), (3, 5, 7)):\n",
    "    hyper_params[key] = param_grid[index]\n",
    "    \n",
    "open_threshold = np.array([0.5091])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если была запущена предыдущая ячейка, то 3 нижеследующих ячейки можно пропустить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oscilo', 'spec']\n",
      "\n",
      "Path # 1\n",
      "Logistic Regression: train acc = 0.922, valid acc = 0.912 (00h 06min 50s)\n",
      "Support Vector Classifier: train acc = 0.987, valid acc = 0.973 (00h 16min 19s)\n",
      "Random Forest Classifier: train acc = 1.000, valid acc = 0.967 (00h 08min 15s)\n",
      "Gradient Boosting Classifier: train acc = 0.998, valid acc = 0.967 (01h 41min 26s)\n",
      "Wall time: 02h 12min 54s\n",
      "\n",
      "Path # 2\n",
      "Logistic Regression: train acc = 0.928, valid acc = 0.914 (00h 06min 05s)\n",
      "Support Vector Classifier: train acc = 0.988, valid acc = 0.968 (00h 16min 03s)\n",
      "Random Forest Classifier: train acc = 1.000, valid acc = 0.966 (00h 08min 18s)\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=17)\n",
    "\n",
    "mode_list = ['oscilo', 'spec']\n",
    "X, y, _, _ = get_data(mode_list)\n",
    "\n",
    "cls_list = ['logit', 'svc', 'rfc', 'gbc']\n",
    "train_proba = {}\n",
    "valid_proba = {}\n",
    "for cls in cls_list:\n",
    "    train_proba[cls] = n_splits * [None]\n",
    "    valid_proba[cls] = n_splits * [None]\n",
    "\n",
    "print(mode_list)\n",
    "print()\n",
    "for i, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n",
    "    print('Path #', i+1)\n",
    "    tic = time.time()\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "    tic_tic = time.time()\n",
    "    logit = LogisticRegression(C=5, penalty='l2', solver='lbfgs', multi_class='ovr', n_jobs=-1, random_state=17)\n",
    "    logit.fit(X_train_scaled, y_train)\n",
    "    train_proba['logit'][i] = logit.predict_proba(X_train_scaled)\n",
    "    valid_proba['logit'][i] = logit.predict_proba(X_valid_scaled)\n",
    "    toc_toc = time.time()\n",
    "    print('Logistic Regression: train acc = %.3f, valid acc = %.3f (%s)' %\n",
    "          (accuracy_score(y_train, [np.argmax(probas) for probas in train_proba['logit'][i]]),\n",
    "           accuracy_score(y_valid, [np.argmax(probas) for probas in valid_proba['logit'][i]]),\n",
    "           time.strftime('%Hh %Mmin %Ss', time.gmtime(toc_toc - tic_tic))))\n",
    "    \n",
    "    tic_tic = time.time()\n",
    "    svc = SVC(C=10, kernel='rbf', gamma='auto', decision_function_shape='ovr', random_state=17, probability=True)\n",
    "    svc.fit(X_train_scaled, y_train)\n",
    "    train_proba['svc'][i] = svc.predict_proba(X_train_scaled)\n",
    "    valid_proba['svc'][i] = svc.predict_proba(X_valid_scaled)\n",
    "    toc_toc = time.time()\n",
    "    print('Support Vector Classifier: train acc = %.3f, valid acc = %.3f (%s)' %\n",
    "          (accuracy_score(y_train, [np.argmax(probas) for probas in train_proba['svc'][i]]),\n",
    "           accuracy_score(y_valid, [np.argmax(probas) for probas in valid_proba['svc'][i]]),\n",
    "           time.strftime('%Hh %Mmin %Ss', time.gmtime(toc_toc - tic_tic))))\n",
    "\n",
    "    tic_tic = time.time()\n",
    "    rfc = OneVsRestClassifier(RandomForestClassifier(n_estimators=250, random_state=17, n_jobs=-1), n_jobs=-1)\n",
    "    rfc.fit(X_train, y_train)\n",
    "    train_proba['rfc'][i] = rfc.predict_proba(X_train)\n",
    "    valid_proba['rfc'][i] = rfc.predict_proba(X_valid)\n",
    "    toc_toc = time.time()\n",
    "    print('Random Forest Classifier: train acc = %.3f, valid acc = %.3f (%s)' %\n",
    "          (accuracy_score(y_train, [np.argmax(probas) for probas in train_proba['rfc'][i]]),\n",
    "           accuracy_score(y_valid, [np.argmax(probas) for probas in valid_proba['rfc'][i]]),\n",
    "           time.strftime('%Hh %Mmin %Ss', time.gmtime(toc_toc - tic_tic))))\n",
    "    \n",
    "    tic_tic = time.time()\n",
    "    gbc = OneVsRestClassifier(GradientBoostingClassifier(n_estimators=150, max_depth=3, random_state=17), n_jobs=-1)\n",
    "    gbc.fit(X_train, y_train)\n",
    "    train_proba['gbc'][i] = gbc.predict_proba(X_train)\n",
    "    valid_proba['gbc'][i] = gbc.predict_proba(X_valid)\n",
    "    toc_toc = time.time()\n",
    "    print('Gradient Boosting Classifier: train acc = %.3f, valid acc = %.3f (%s)' %\n",
    "          (accuracy_score(y_train, [np.argmax(probas) for probas in train_proba['gbc'][i]]),\n",
    "           accuracy_score(y_valid, [np.argmax(probas) for probas in valid_proba['gbc'][i]]),\n",
    "           time.strftime('%Hh %Mmin %Ss', time.gmtime(toc_toc - tic_tic))))\n",
    "    \n",
    "    toc = time.time()\n",
    "    print('Wall time: %s' % time.strftime('%Hh %Mmin %Ss', time.gmtime(toc - tic)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = np.linspace(0.0, 1.0, 11)\n",
    "valid_accuracy = np.zeros(tuple([n_splits] + [11]*(len(cls_list) - 1)))\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "    \n",
    "    for j, alpha in enumerate(param_grid):\n",
    "        for k, beta in enumerate(param_grid):\n",
    "            for m, gamma in enumerate(param_grid):\n",
    "                ensemble_proba = gamma*(beta*(alpha*valid_proba['logit'][i] \\\n",
    "                                              + (1.0 - alpha)*valid_proba['rfc'][i]) \\\n",
    "                                        + (1.0 - beta)*valid_proba['gbc'][i]) \\\n",
    "                                 + (1.0 - gamma)*valid_proba['svc'][i]\n",
    "                valid_accuracy[i][j][k][m] = \\\n",
    "                    accuracy_score(y_valid, [np.argmax(probas) for probas in ensemble_proba])\n",
    "\n",
    "#'logit', 'rfc', 'gbc', 'svc'\n",
    "mean_valid_accuracy = np.mean(valid_accuracy, axis=0)\n",
    "arg_max_ravel = np.argmax(mean_valid_accuracy)\n",
    "idx = np.unravel_index(arg_max_ravel, tuple([11]*(len(cls_list) - 2)))\n",
    "print(mean_valid_accuracy[idx], idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {}\n",
    "for key, index in zip(('alpha', 'beta', 'gamma'), idx):\n",
    "    hyper_params[key] = param_grid[index]\n",
    "\n",
    "false_proba_predict = []\n",
    "for i, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "    \n",
    "    ensemble_proba = hyper_params['gamma']*(hyper_params['beta']*(hyper_params['alpha']*valid_proba['logit'][i] \\\n",
    "                                                                  + (1.0 - hyper_params['alpha'])*valid_proba['rfc'][i]) \\\n",
    "                                            + (1.0 - hyper_params['beta'])*valid_proba['gbc'][i]) \\\n",
    "                     + (1.0 - hyper_params['gamma'])*valid_proba['svc'][i]\n",
    "\n",
    "    ensemble_proba = np.array(ensemble_proba)\n",
    "    ensemble_class = np.array([np.argmax(probas) for probas in ensemble_proba])\n",
    "    false_predict_proba += np.array([ensemble_proba[i, cls] for i, cls in enumerate(ensemble_class)]) \\\n",
    "                           [y_valid != ensemble_class].tolist()\n",
    "\n",
    "open_threshold = np.median(false_predict_proba)\n",
    "print('threshold = %.4f' % open_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = get_data(mode_list)\n",
    "\n",
    "test_close_index = (df_test['Class'] != 'unknown').values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "logit = LogisticRegression(C=5, penalty='l2', solver='lbfgs', multi_class='ovr', n_jobs=-1, random_state=17)\n",
    "logit.fit(X_train_scaled, y_train)\n",
    "logit_proba = logit.predict_proba(X_test_scaled)\n",
    "\n",
    "svc = SVC(C=10, kernel='rbf', gamma='auto', decision_function_shape='ovr', random_state=17, probability=True)\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "svc_proba = svc.predict_proba(X_test_scaled)\n",
    "\n",
    "rfc = OneVsRestClassifier(RandomForestClassifier(n_estimators=250, random_state=17, n_jobs=-1), n_jobs=-1)\n",
    "rfc.fit(X_train, y_train)\n",
    "rfc_proba = rfc.predict_proba(X_test)\n",
    "\n",
    "gbc = OneVsRestClassifier(GradientBoostingClassifier(n_estimators=150, max_depth=3, random_state=17), n_jobs=-1)\n",
    "gbc.fit(X_train, y_train)\n",
    "gbc_proba = gbc.predict_proba(X_test)\n",
    "\n",
    "ensemble_proba = hyper_params['gamma']*(hyper_params['beta']*(hyper_params['alpha']*logit_proba \\\n",
    "                                                              + (1.0 - hyper_params['alpha'])*rfc_proba) \\\n",
    "                                        + (1.0 - hyper_params['beta'])*gbc_proba) \\\n",
    "                 + (1.0 - hyper_params['gamma'])*svc_proba\n",
    "        \n",
    "ensemble_class_close = np.array([np.argmax(probas) for probas in ensemble_proba])\n",
    "\n",
    "print(accuracy_score(y_test[test_close_index], ensemble_class_close[test_close_index]))\n",
    "cfm_close = confusion_matrix(ensemble_class_close[test_close_index], y_test[test_close_index])\n",
    "pd.DataFrame(np.vstack((cfm_close, cfm_close.sum(axis=0),\n",
    "                        [round(cfm_close[i, i] / cfm_close.sum(axis=0)[i], ndigits=2) for i in range(cfm_close.shape[1])])),\n",
    "             index=list(d_class.keys())[:-1] + ['Total', 'Recall'],\n",
    "             columns=[str[:4] for str in list(d_class.keys())[:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_result_close = pd.concat([df_test['File'],\n",
    "                            pd.Series([np.max(proba) for proba in ensemble_proba], name='Score'),\n",
    "                            pd.Series([d_class_inv[cls] for cls in ensemble_class_close], name='Class')], axis=1)\n",
    "df_result_close.to_csv('./result_close.txt', sep='\\t', header=False, index=False, float_format='%.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_class_open = ensemble_class_close.copy()\n",
    "ensemble_class_open[[np.max(proba) for proba in ensemble_proba] < open_threshold] = d_class['unknown']\n",
    "\n",
    "print(accuracy_score(y_test, ensemble_class_open))\n",
    "cfm_open = confusion_matrix(ensemble_class_open, y_test)\n",
    "pd.DataFrame(np.vstack((cfm_open, cfm_open.sum(axis=0),\n",
    "                        [round(cfm_open[i, i] / cfm_open.sum(axis=0)[i], ndigits=2) for i in range(cfm_open.shape[0])])),\n",
    "             index=list(d_class.keys()) + ['Total', 'Recall'],\n",
    "             columns=[str[:4] for str in list(d_class.keys())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_result_open = pd.concat([df_test['File'],\n",
    "                            pd.Series([np.max(proba) for proba in ensemble_proba], name='Score'),\n",
    "                            pd.Series([d_class_inv[cls] for cls in ensemble_class_open], name='Class')], axis=1)\n",
    "df_result_open.to_csv('./result_open.txt', sep='\\t', header=False, index=False, float_format='%.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
